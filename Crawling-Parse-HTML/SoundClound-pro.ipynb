{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e01e451",
   "metadata": {
    "id": "1e01e451"
   },
   "source": [
    "<div style=\"text-align: center;\"><font size=\"5\"><strong>LIBRARY</strong></font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afecfc90",
   "metadata": {
    "id": "afecfc90"
   },
   "source": [
    "+ import time: Set the sleep time of each scroll when the program send the key down to site.\n",
    "+ import requests: Request the web page.\n",
    "+ import pandas as pd: Work with csv.\n",
    "+ import urllib.robotparser: To know can the program crawl the web page.\n",
    "+ from bs4 import BeautifulSoup: Work with html, find element and the value.\n",
    "+ from selenium import webdriver: Because Soundclound blocked with javascipt so that we have to use this library to call chrome driver to get the page source after javascript loaded.\n",
    "+ from selenium.webdriver.common.keys import Keys: Send the key to the website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6400e3ca",
   "metadata": {
    "id": "6400e3ca"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "import urllib.robotparser\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad97d0a5",
   "metadata": {
    "id": "ad97d0a5"
   },
   "source": [
    "<div style=\"text-align: center;\"><font size=\"5\"><strong>GLOBAL VARIABLE</strong></font></div>\n",
    "\n",
    "+ limit: The quantity of users.\n",
    "+ uniq_user: The list created to store set of unique users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e83fa2",
   "metadata": {
    "id": "30e83fa2"
   },
   "outputs": [],
   "source": [
    "uniq_user=[] #unique user list\n",
    "limit=1001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f81f91c",
   "metadata": {
    "id": "2f81f91c"
   },
   "source": [
    "<div style=\"text-align: center;\"><font size=\"5\"><strong>CHECK ROBOTS.TXT FILE</strong></font></div>\n",
    "\n",
    "+ This function to check the robots text file follow this link: https://soundcloud.com/discover/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdeb1e87",
   "metadata": {
    "id": "bdeb1e87",
    "outputId": "198cd215-f43b-44a2-82b4-cfd701f46c48"
   },
   "outputs": [],
   "source": [
    "rp = urllib.robotparser.RobotFileParser(url='https://soundcloud.com/robots.txt')\n",
    "rp.read()\n",
    "rp.can_fetch('*','https://soundcloud.com/discover/' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89b384f",
   "metadata": {
    "id": "b89b384f"
   },
   "source": [
    "<div style=\"text-align: center;\"><font size=\"5\"><strong>FUNCTIONS</strong></font></div>\n",
    "\n",
    "+ <font size=\"3\">We used a combined HTML parsing method using the selenium library to get the data from website soundcloud.com.\n",
    "Here are some functions used in the program:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98137149",
   "metadata": {},
   "source": [
    "+ page_scroll_down(driver, number_of_key_down=8):This function help us use selenium to scroll specified web page with two argument driver (driver of link to specified web page) and number_of_key_down(number of page scrolls)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff3d5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_scroll_down(driver, number_of_key_down=8):\n",
    "    page_body = driver.find_element_by_tag_name('body')\n",
    "    num_of_pagedowns = number_of_key_down\n",
    "    while num_of_pagedowns:\n",
    "        page_body.send_keys(Keys.PAGE_DOWN)\n",
    "        time.sleep(1)\n",
    "        num_of_pagedowns-=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228c0823",
   "metadata": {
    "id": "228c0823"
   },
   "source": [
    "+ clone_and_save_url_to_playlist(): This function help us filter and save links to top playlist. We have filtered the featured playlists in the link 'https://soundcloud.com/discover/', and saved their links to a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae12f31",
   "metadata": {
    "id": "6ae12f31"
   },
   "outputs": [],
   "source": [
    "def clone_and_save_url_to_playlist():\n",
    "    url = 'https://soundcloud.com/discover/'\n",
    "    driver = webdriver.Chrome('chromedriver.exe')\n",
    "    driver.get(url)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    driver.find_element_by_xpath(\"//button[@id='onetrust-accept-btn-handler']\").click()\n",
    "    \n",
    "    for i in range(1, 9):\n",
    "        for j in range(10):\n",
    "            driver.find_element_by_xpath(f'//*[@id=\"content\"]/div/div/div[1]/div[3]/div/ul/li[{i}]/div/div[2]/div[2]/button').click()\n",
    "    \n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html)\n",
    "    url_to_playlist = soup.find_all('a',{'class':'playableTile__artworkLink'})\n",
    "    # save url to playlist to text file\n",
    "    with open('sc_url_to_playlist.txt','w') as writer:\n",
    "        for href in url_to_playlist:\n",
    "            writer.write('https://soundcloud.com' + href['href'] + '\\n')\n",
    "    driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8d67d1",
   "metadata": {
    "id": "3a8d67d1"
   },
   "source": [
    "+ get_url_to_playlist():This function help us get the link from a csv file with one argument is file_name (name of the file we wanna read the data frame)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12ce4d4",
   "metadata": {
    "id": "e12ce4d4"
   },
   "outputs": [],
   "source": [
    "def get_url_to_playlist(file_name='sc_url_to_playlist.txt'):\n",
    "    urls = []\n",
    "    # get url\n",
    "    with open(file_name) as reader:\n",
    "        for line in reader:\n",
    "            urls.append(line.rstrip())\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60514ced",
   "metadata": {
    "id": "60514ced"
   },
   "source": [
    "+ crawl_track_info(url_to_profile, user_name, driver, writer):This function help us crawl track info from each user's track page and save info to csv file \n",
    "    We go to the track page's link, then scroll to the bottom of the page to get all the links to each track.\n",
    "    After that, we and each track to get information such as name, author, views, likes, link to track,....\n",
    "    If after filtering, but the necessary information is not available, it will return None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63790e0",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1636725628397,
     "user": {
      "displayName": "Đăng Kiều Hải",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiZXqQZH9lTpv-aj7v7v84Yyc-_GQNAZXRi_LCB=s64",
      "userId": "04370342150479985233"
     },
     "user_tz": -420
    },
    "id": "e63790e0"
   },
   "outputs": [],
   "source": [
    "def crawl_track_info(url_to_profile, user_name, driver, writer):\n",
    "    \n",
    "    tracks_tab_url = url_to_profile + '/tracks'\n",
    "    driver.get(tracks_tab_url)\n",
    "    page_scroll_down(driver, 3)\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html)\n",
    "    track_url = soup.find_all('a',{'class':'soundTitle__title'})\n",
    "        \n",
    "    if len(track_url) == 0: writer.write(f'None>>{user_name}>>None>>None>>None>>>None>>>None>>>None\\n')\n",
    "    else:\n",
    "        for href in track_url:\n",
    "            url_track_info = 'https://soundcloud.com' + href['href'].strip()\n",
    "            driver.get(url_track_info)\n",
    "            time.sleep(1)\n",
    "            html = driver.page_source\n",
    "            soup = BeautifulSoup(html)\n",
    "            \n",
    "            track_info = ''\n",
    "            try: track_info += (soup.find('div', {'class':'soundTitle__titleHeroContainer'})).find('h1',{'class':'soundTitle__title'}).text.replace('\\n', '') + '>>'\n",
    "            except: track_info += 'None>>'\n",
    "            track_info += user_name + '>>'\n",
    "            try: track_info += soup.find('a',{'class':'sc-tag-large'}).text.replace('\\n', '') + '>>'\n",
    "            except: track_info += 'None>>'\n",
    "            track_info += f'{url_track_info}>>'\n",
    "            try: track_info += (soup.find('span',{'class':'sc-ministats sc-ministats-medium sc-ministats-plays sc-text-secondary'})).find('span',{'class': 'sc-visuallyhidden'}).text.split(' ')[0] + '>>'\n",
    "            except: track_info += 'None>>'\n",
    "            try: track_info += soup.find('a',{'class':'sc-ministats sc-ministats-medium sc-ministats-likes sc-link-secondary'}).text.replace('View all likes', '').replace('\\n', '') + '>>'\n",
    "            except: track_info += 'None>>'\n",
    "            try: track_info += soup.find('a',{'class':'sc-ministats sc-ministats-medium sc-ministats-reposts sc-link-secondary'}).text.replace('View all reposts', '').replace('\\n', '')\n",
    "            except: track_info += 'None'\n",
    "                \n",
    "            writer.write(track_info + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f882116f",
   "metadata": {
    "id": "f882116f"
   },
   "source": [
    "+ crawl_playlist_info(url_to_profile, user_name, driver, writer): crawl playlist info from each user's playlist page and save info to csv file \n",
    "    We go to the playlist page's link, then scroll to the bottom of the page to get all the links to each playlist.\n",
    "    After that, we and each playlist to get information such as name, author, views, likes, link to playlist,....\n",
    "    If after filtering, but the necessary information is not available, it will return None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28c1542",
   "metadata": {
    "id": "e28c1542"
   },
   "outputs": [],
   "source": [
    "def crawl_playlist_info(url_to_profile, user_name, driver, writer):\n",
    "    playlist_tab_url = url_to_profile + '/sets'\n",
    "    driver.get(playlist_tab_url)\n",
    "    page_scroll_down(driver, 3)\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html)\n",
    "    playlist_url = soup.find_all('a',{'class':'sound__coverArt'})\n",
    "\n",
    "    if len(playlist_url) == 0:\n",
    "        writer.write(f'None>>None>>{user_name}>>None>>None>>None\\n')\n",
    "    else:\n",
    "        for href in playlist_url:\n",
    "            url_playlist_info = 'https://soundcloud.com' + href['href'].strip()\n",
    "            driver.get(url_playlist_info)\n",
    "            time.sleep(1)\n",
    "            html = driver.page_source\n",
    "            soup = BeautifulSoup(html)\n",
    "            \n",
    "            playlist_info = ''\n",
    "            try: playlist_info += (soup.find('div', {'class':'soundTitle__titleHeroContainer'})).find('h1',{'class':'soundTitle__title'}).text.replace('\\n', '') + '>>'\n",
    "            except: playlist_info += 'None>>'\n",
    "            try: playlist_info += soup.find('a',{'class':'sc-tag-large'}).text.replace('\\n', '') + '>>'\n",
    "            except: playlist_info += 'None>>'\n",
    "            try: playlist_info += f'{user_name}>>'\n",
    "            except: playlist_info += 'None>>'\n",
    "            try: playlist_info += f'{url_playlist_info}>>'\n",
    "            except: playlist_info += 'None>>'\n",
    "            try: playlist_info += soup.find('div',{'class': 'genericTrackCount__title sc-text-h1'}).text.strip() + '>>'\n",
    "            except: playlist_info += 'None>>'\n",
    "            try: playlist_info += (soup.find('ul',{'class':'sc-ministats-group-right'})).find('a',{'class':'sc-ministats-likes'}).text.replace('\\n', '').replace('View all likes','') + '>>' #like\n",
    "            except: playlist_info += 'None>>'\n",
    "            try: playlist_info += (soup.find('ul',{'class':'sc-ministats-group-right'})).find('a',{'class':'sc-ministats-reposts'}).text.replace('\\n', '').replace('View all reposts','') #repost\n",
    "            except: playlist_info += 'None'\n",
    "                \n",
    "            writer.write(f'{playlist_info}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49caed4",
   "metadata": {
    "id": "b49caed4"
   },
   "source": [
    "+ HTML_Parser():get data like user info, track info and playlist info\n",
    "    We get link featured playlists and filter users with hit tracks.\n",
    "    Then, go to each user's profile to get info and links to their tracks and playlists.\n",
    "    Then we get track info and playlist info using other functions.\n",
    "    After that, we will save profile info, track info and playlist info to the csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb55391",
   "metadata": {
    "id": "ffb55391"
   },
   "outputs": [],
   "source": [
    "def HTML_Parser():\n",
    "    urls = get_url_to_playlist()\n",
    "    driver = webdriver.Chrome('chromedriver.exe')\n",
    "    with open('user.csv', 'w', encoding='utf-8') as writer1:\n",
    "        with open('track_list.csv', 'w', encoding='utf-8') as writer2:\n",
    "            with open('play_list.csv', 'w', encoding='utf-8') as writer3:\n",
    "                count=0\n",
    "                writer1.write('user_name>>follower>>following>>tracks\\n')\n",
    "                writer2.write('name of track>>user_name>>genre>>link to track>>playbacks>>likes of track>>reposts of track\\n')\n",
    "                writer3.write('name of playlist>>genre>>writer>>link to playlist>>track count>>likes of playlist>>reposts of playlist\\n')\n",
    "                for url in urls:\n",
    "                    \n",
    "                    # Go to user profile and crawl user info\n",
    "                    driver.get(url)\n",
    "                    page_scroll_down(driver, 8)\n",
    "                    html = driver.page_source\n",
    "                    soup = BeautifulSoup(html)\n",
    "                    list_href_of_user_profile = soup.find_all('a',{'class':'trackItem__username'})\n",
    "                    if len(list_href_of_user_profile) == 0:\n",
    "                        print('Network error at this session please start again or wait for the next!')\n",
    "                    else:\n",
    "                        for href in list_href_of_user_profile:\n",
    "                            if href not in uniq_user:\n",
    "                                uniq_user.append(href)\n",
    "                                url_to_profile = 'https://soundcloud.com' +  href['href']\n",
    "                                driver.get(url_to_profile)\n",
    "                                time.sleep(2)\n",
    "                                html = driver.page_source\n",
    "                                soup = BeautifulSoup(html)\n",
    "                                user_name = ''\n",
    "                                try: user_name = (soup.find('div',{'class':'profileHeaderInfo__content'})).find('h2',{'class': 'profileHeaderInfo__userName'}).text.replace('\\n', '')[4:-4]\n",
    "                                except: user_name = 'None'\n",
    "                                user_profile = user_name + '>>'\n",
    "                                #============================================\n",
    "                                user_stats = soup.find_all('div',{'class':'infoStats__value'})\n",
    "                                if len(user_stats) == 0: user_profile += 'None>>None>>None>>'\n",
    "                                else:\n",
    "                                    for stat in user_stats: user_profile += stat.text.replace('\\n', '') + '>>'\n",
    "                                writer1.write(user_profile[:-2] + '\\n')    \n",
    "                            # Go to track tab and crawl track info\n",
    "                                crawl_track_info(url_to_profile, user_name, driver, writer2)\n",
    "                            #===========================================\n",
    "                            # Go to playlist tab and crawl playlist info\n",
    "                                crawl_playlist_info(url_to_profile, user_name, driver, writer3)\n",
    "                            #===========================================\n",
    "                                count+=1\n",
    "                                if count==limit:return\n",
    "    driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2b24eb",
   "metadata": {
    "id": "3a2b24eb"
   },
   "source": [
    "+ SC_read_data(): This function help us load the data frame from a csv file with two argument is file_name (name of the file we wanna read the data frame) and sep (the separate for every element of csv file). We use pandas to read csv by read_csv() function, load the file_name and sep into that and store the data frame into variable named data and return the data. In the data frame of the output we have three file of data is:\n",
    "    + user.csv\n",
    "    + track_list.csv\n",
    "    + play_list.csv\n",
    "\n",
    "+ NOTICE:\n",
    "    + The separate default is >> because maybe the string is included space or ; so we use >> not sure for 100% but it can get 90% of the result.\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efac2fd8",
   "metadata": {
    "id": "efac2fd8"
   },
   "outputs": [],
   "source": [
    "def SC_read_data(file_name='track_list.csv', sep='>>'):\n",
    "    data = pd.read_csv(file_name, sep=sep)\n",
    "    return data\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f936cc",
   "metadata": {
    "id": "56f936cc"
   },
   "source": [
    "<div style=\"text-align: center;\"><font size=\"5\"><strong>MAIN</strong></font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d04363",
   "metadata": {
    "id": "c4d04363"
   },
   "source": [
    "+ Lines below are used as the main function to run the program from A-Z.\n",
    "\n",
    "+ NOTICE:\n",
    "    + So sory if the data get the name of track is none: During the progress it maybe the weak network affect that.\n",
    "    + Time to run < 1000 record is so long please consider before the progress or you can change the limit variable on the Variable cell to limit the test case.\n",
    "    + Please choose Kernel ==> Restart & Run All."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d94844d",
   "metadata": {
    "id": "2d94844d"
   },
   "source": [
    "# Parse the web page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac0ff4b",
   "metadata": {
    "id": "bac0ff4b",
    "outputId": "56a917b2-f0bb-4efe-cbb6-77c2505c4738",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# clone_and_save_url_to_playlist()\n",
    "HTML_Parser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1793ac1",
   "metadata": {
    "id": "b1793ac1"
   },
   "source": [
    "# Ten first line of the data frame user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90354701",
   "metadata": {
    "id": "90354701",
    "outputId": "071c79b5-daca-4730-ea21-606d484ec050",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = SC_read_data('user.csv')\n",
    "df[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388a41e7",
   "metadata": {},
   "source": [
    "# Info of the data frame user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7b3556",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69416eff",
   "metadata": {},
   "source": [
    "# Ten first line of the data frame tracklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc7e011",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = SC_read_data('track_list.csv')\n",
    "df[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1313c15c",
   "metadata": {},
   "source": [
    "# Info of the data frame tracklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a7268f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf96b2ed",
   "metadata": {},
   "source": [
    "# Ten first line of the data frame playlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c780938",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = SC_read_data('play_list.csv')\n",
    "df[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f1969c",
   "metadata": {
    "id": "37f1969c"
   },
   "source": [
    "# Info of the data frame playlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835ac28f",
   "metadata": {
    "id": "835ac28f",
    "outputId": "aecdddd0-e23c-4c2e-8569-82ceaec92da3"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "SoundClound-pro.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
